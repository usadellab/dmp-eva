# DMP Evaluation Criteria

## Version 1.0 | November 2025

# 1\. Introduction

Data Management Plans (DMPs) are a core responsibility of Research Data Management (RDM) professionals—namely, data stewards and data officers. Increasingly, both research institutions and funding agencies require DMPs as formal components of research proposals, reflecting a broader shift toward transparency, reproducibility, and adherence to the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. As a result, the ability to prepare comprehensive and high-quality DMPs has become an essential skill in modern research management.

However, evaluating DMPs remains a time-consuming and complex task that extends across all phases of the research lifecycle. To provide consistent and constructive support, data stewards need clear, well-defined criteria that describe what constitutes a good DMP. Yet, since stewards often serve researchers across diverse disciplines, they may not possess deep expertise in every scientific area they support. This makes it crucial to establish an evaluation framework that is both broadly applicable across disciplines and funders (Table 1\) and specific enough to guide researchers and reviewers in producing meaningful, phase-appropriate DMPs.

Recognizing this need, the infra-dmp DMP Evaluation Criteria Working Group has developed a flexible and scalable set of DMP evaluation criteria aligned with real-world project lifecycles. Building upon and extending the Science Europe DMP evaluation rubric, these criteria are designed to adapt to the evolving nature of research projects. DMPs are understood here as living documents—not one-off administrative tasks, but dynamic plans that evolve with the progress of the project. Accordingly, the evaluation framework distinguishes between different project phases and asks a central question: Is the information provided sufficient for this stage of the project?

This phase-sensitive approach allows for a more accurate and constructive assessment by treating DMPs as iterative planning tools rather than static deliverables. It supports both flexibility and accountability, recognizing that early-stage plans will necessarily differ from mid- or end-stage reporting.

This staged structure mirrors the expectations of major funding agencies, thereby promoting transparency, accountability, and continuous improvement in data management. Examples of funding organizations that mandate DMPs are shown in Table 1\.

To operationalize this phrased approach, Table 2 presents the evaluation framework across three stages—proposal/early stage, mid-project, and end-project—each corresponding to a specific point in the research lifecycle.

At the proposal stage, researchers outline initial strategies for managing data, ensuring alignment with FAIR principles and funder requirements.

The mid-project stage provides a checkpoint to review, monitor, and refine data management practices based on actual project developments.

The end-project stage focuses on reporting the outcomes of these efforts, ensuring that data are properly shared, preserved, and documented for long-term accessibility and reuse.

While the general evaluation framework provides a consistent structure, we also recognize the importance of domain-specific expertise. Different research areas may have distinct data types, standards, and workflows. To accommodate this, we are developing discipline-specific guidance that complements the general criteria. The initial guidance covers physics, plant biology, and materials science, with plans to expand into other domains. We actively encourage contributions from additional disciplines, ensuring that this framework remains a living, community-driven resource.

Importantly, this evaluation framework can also serve funding agencies and proposal reviewers. Reviewers often have limited time and may not be specialists in data management. By offering a standardized, transparent, and easily interpretable structure for assessing DMP quality, the framework supports fairer, more consistent evaluations across different contexts. Agencies such as the Deutsche Forschungsgemeinschaft (DFG) and others could adopt or adapt this framework to improve coherence and comparability in DMP review processes.

Ultimately, our goal is to empower both data stewards and researchers. By providing clear, adaptable, and lifecycle-aware evaluation criteria, we aim to facilitate better DMPs—ones that are not only compliant but genuinely useful in guiding the management, sharing, and preservation of research data across disciplines, institutions, and funding contexts.

# 2\. Funding Bodies’ Requirements on DMP Update

# Table 1\. Funding Body Update Requirements Overview

| Funder | Initial Mandatory Submission | Update Trigger Type | Mandatory Update Timeline | Source Confirmation |
| ----- | ----- | ----- | ----- | ----- |
| Horizon Europe (Europe) | Short DMP in proposal; full DMP deliverable \~ Month 6 of funded project | Periodic / Milestone (living document) | For projects \>12 months: updated at mid-project AND final version by end of project | [“How to comply with Horizon Europe RDM mandate” – OpenAIRE](https://www.openaire.eu/how-to-comply-with-horizon-europe-mandate-for-rdm?utm_source=chatgpt.com) |
| Horizon 2020 (Europe) | Full DMP as deliverable (≈ Month 6\) for many projects | Periodic / Event-Triggered (significant changes) | Updated when major changes occur; at minimum by final review of project | [OpenAIRE – “How to create a Data Management Plan”](https://www.openaire.eu/how-to-create-a-data-management-plan) |
| MSCA (Europe) | DMP deliverable \~ Month 6 (or as per action type) | Milestone / Project-lifetime update | Revised toward end of life of project (and mid-project if \>12 months) | (Uses same HE policy) |
| BMEL (Germany) | DMP (Forschungsdatenmanagementplan) required at application stage | Reporting Milestones (Zwischen- & Final-Reports) | Must be updated for interim (“Zwischenbericht”) and final (“Abschlussbericht”) reports | [FU Berlin – Handreichung Förderer: BMEL](https://www.fu-berlin.de/sites/forschungsdatenmanagement/materialien/handreichungen/forschungsfoerderer/bmel.html?utm_source=chatgpt.com) |
| DFG (Germany) | RDM section in proposal (Section 2.4) | Proposal stage only | No explicit mandatory post-award update timeline | [HAWK – Checkliste DMP nach Mittelgebern (incl. DFG)](https://fdm-nds-haw.hawk.de/sites/default/files/2023-10/04_Checkliste_DMP_nach_Mittelgebern.pdf) |
| BMBF (Germany) | DMP attached to application (where required) | Highly variable (call‐dependent) | Typically no mandated update unless specified in individual call | [FU Berlin – Förderer Handreichung (incl. BMBF)](https://www.fu-berlin.de/sites/forschungsdatenmanagement/materialien/handreichungen/forschungsfoerderer/index.html?utm_source=chatgpt.com) |
| NSF (USA) | Data Management Plan (2 pages) at proposal submission | Scope change / Future accountability | No scheduled mandatory updates defined; compliance via reports | [RDMkit – NSF section](https://rdmkit.elixir-europe.org/data_management_plan) |
| BBSRC (UK) | DMP with proposal application (mandatory) | PI-defined / Self-audit | No formal timeline for updates specified; researchers should revise during project as needed | [Uni Mannheim – FDM Info BBSRC](https://www.bib.uni-mannheim.de/en/lehren-and-forschen/forschungsdatenzentrum/fdm-informationen/datenmanagementplan-1/?utm_source=chatgpt.com) |
| Volkswagen Foundation (Germany) | DMP at application stage for data-intensive projects | Proposal stage only | No explicit mandated update timeline in publicly available policy extracts | [HAWK Checkliste DMP nach Mittelgebern (incl. VW-Stiftung)](https://fdm-nds-haw.hawk.de/sites/default/files/2023-10/04_Checkliste_DMP_nach_Mittelgebern.pdf) |
| CZS (Germany) | “Forschungsdatenkonzept” (data management concept) required in application (max \~2 pages) | Proposal stage only | No explicit update timeline stated in the general guidelines for data-management concept | [CZS Ausschreibung Stiftungsprofessuren – Richtlinien](https://www.carl-zeiss-stiftung.de/fileadmin/mediamanager/downloads/applications/20250318_Ausschreibung_Stiftungprofessuren_Uni_2025-2.pdf) |

### Notes / Clarifications

* For Horizon Europe: The requirement that the DMP is treated as a “living document” and “regularly updated” is explicitly stated.  
* For BMEL: The requirement to update the DMP with each interim and final report is clearly documented.  
* For DFG: Several guidance pages (e.g., German university libraries) note that while a data management section is required in the proposal, “an update during the project is not (yet) mandatory”.  
* For NSF, BBSRC, BMBF and Volkswagen Foundation: The policies vary (or are less specific) about update timelines — they require a plan, but unlike HE/BMEL they do not mandate fixed update checkpoints in the publicly available policy extracts.  
* For Carl-Zeiss-Stiftung: The call documentation clearly requires a “Forschungsdatenkonzept” as part of the application, but I did not locate a reference in those documents to mandatory mid-project or end-project updates of that concept.  
* Note: Even where update checkpoints aren’t mandatory, many funders allow or expect the DMP/concept to be revised “if significant changes occur”. Researchers should still plan for revisions ideally.

# 3\. Phrased Evaluation Criteria

The following criteria are organized by project phase to reflect the evolving nature of data management planning.Information provided should be sufficient for the current project stage. Earlier stage criteria remain relevant and should be monitored (mid-project) or reported (end-project); Criteria support both planning (early) and accountability (later stages)

Table 2\. General Evaluation Criteria for DMP

| Proposal/Early Stage | Mid-Project | End-Project |
| ----- | ----- | ----- |
| 1   DATA DESCRIPTION AND COLLECTION OR RE-USE OF EXISTING DATA 1aHow will new data be collected or produced and/or how will existing data be re-used? |  |  |
| For new data collection or production: Describe the data in terms of scientific context and technical perspective Specify the methods, tools, instruments, and software to be used for data collection Justify collecting or producing new data (see Section 1b) For reuse of existing data: Describe the reused data (type, scope, and relevance) Explain the purpose of reusing the data (e.g., validation, comparison, integration) Mention sources, access conditions, and permissions (e.g., database name, DOI, publication, software, licensing, embargoes, access restrictions) | Confirm which data types have been produced or reused so far Update formats and tools used, noting any changes Include information on data provenance Update documentation to show growth and technical advances as data is generated | List final data types and formats collected or reused Provide justification for deviations from the original plan |
| 1b What data (for example the kind, formats, and volumes) will be collected or produced? |  |  |
| List expected data types, formats (e.g., open vs. proprietary), and estimated volume (size/number of files) If proprietary or less-common formats are used, provide justification including any limitations for reuse or access If the project will not produce new data, clearly state this with an explanation (see Section 1a) | Monitor RDM pipelines and data analysis workflows Provide technical updates on actual volume, types, and formats produced, noting any technical constraints encountered Describe data conversion strategies if data will be transformed between formats (e.g., raw to processed formats, proprietary to open) | Archive the final complete set of technical documentation and metadata files Report on how the final data's technical fidelity can be ensured for reuse |
| 2   DOCUMENTATION AND DATA QUALITY 2a What metadata and documentation (for example the methodology of data collection and way of organising data) will accompany the data? Are method- or discipline specific standards being used? (if so name them) |  |  |
| Describe the types of metadata to be provided, including both descriptive metadata (e.g., title, creator, keywords, project information) and process-related metadata (e.g., experimental or simulation conditions, instrumentation settings, sampling methods, software versions) List suitable candidates for community metadata standards Confirm chosen metadata standards align with community best practices and are suitable for the data type and discipline Mention tools or platforms used for creating and managing metadata (e.g., ELNs, lab management systems, automated metadata capture tools) Specify the format(s) in which metadata will be stored or shared (e.g., JSON-LD, XML, CSV, RDF) | Report which metadata standards or tools are being used Describe ongoing quality assurance and quality control (QA/QC) procedures Explain the rationale for selecting specific metadata standards If applicable, describe data models or metadata schemas that the project has developed or adopted, including justification why existing ontologies or vocabularies are not sufficient Describe how the metadata are stored and shared | Provide evidence of complete documentation (e.g., metadata files, ELN snapshots) Summarize QA outcomes Describe how metadata will be maintained after the end of the project |
| 2b What data quality control measures will be used? |  |  |
| Describe the approach to ensuring data quality, including verification and validation procedures Describe measures to track changes and manage versions of data and metadata Outline quality control steps during data collection and processing (e.g., consistency checks, use of standard procedures, comparison with reference or benchmark data, comparison with control samples) If applicable, indicate whether the data collection was reviewed by an ethics committee (see Section 4c) | Report which measures were taken to ensure data quality | Report on the quality control measures taken Combine quality control with other criteria such as ethical, legal, and preservation requirements |
| 3   STORAGE AND BACKUP DURING THE RESEARCH PROCESS 3a How will data and metadata be stored and backed up during the research? |  |  |
| Provide a storage concept describing where data and metadata will reside during the project (institutional servers, repositories, cloud) Explain backup or versioning procedures, including methods, frequency, and responsibility Note whether institutional IT or RDM services provide storage or backup support | Report where data and metadata are actually stored and whether this deviates from the initial plan Document active backup or versioning procedures, including verification or log checks Describe the organizational structure for data and associated documentation | List the final storage and backup arrangements for all project data and metadata |
| 3b Who needs access to the data? Is there legal aspects that have to be considered (to limit access)? |  |  |
| Identify who is expected to have access to data and metadata during the project (e.g., PhD students, collaborators, data stewards) Note any anticipated access restrictions and their reasons (e.g., data sensitivity, confidentiality, IP) Outline how access will be granted and documented (e.g., user accounts, project folder structure, institutional authentication) | Update the list of individuals or teams currently having access, reflecting staff or collaboration changes Specify intended levels of access (e.g., read-only, edit, administrative) for different users or groups Confirm that permissions and access rights are being actively managed (e.g., regular review of user accounts) Note how access monitoring or audit logs are maintained, either by the project or institutional IT | Provide a final list or summary of who had access during the project and how it was managed State how ongoing access requests (e.g., from external users, or new group members) will be handled after project completion |
| 3c How will data security and protection of sensitive data be taken care of during the research? |  |  |
| Identify whether sensitive, personal, or confidential data will be handled and what makes them sensitive (e.g., personal identifiers, proprietary results, trade secrets; see Section 4a) Describe the planned security approach for data storage and transfer (e.g., encrypted drives, secured servers, controlled network access, physical restrictions) Explain how access to sensitive data will be restricted (e.g., password protection, role-based permissions) Mention relevant institutional or national data-protection policies (e.g., GDPR compliance statement or link) Specify which support services or responsible offices (e.g., data-protection officer, IT security) will assist the project Outline a basic incident-response or recovery plan in case of data loss or breach | Confirm which sensitive data have actually been collected or processed and where they are stored Describe how security measures have been implemented (e.g., encryption enabled, access logs maintained) Show that data-protection responsibilities are clearly assigned within the project or to institutional services Report any security incidents or near misses and the steps taken to address them Indicate how data transfers or collaborations are secured (e.g., VPN, encrypted exchange platforms) | Confirm that all sensitive data are securely stored, anonymized, or deleted according to institutional and legal requirements Document the final security configuration and provide references to institutional policies or repository standards followed Summarize how security and privacy obligations were fulfilled, including GDPR or contractual requirements Describe how long-term archived or retained data will remain protected (e.g., repository with certified security standards) |
| 4   LEGAL AND ETHICAL REQUIREMENTS, CODES OF CONDUCT 4a If personal data are processed, how will compliance with legislation on personal data and security be ensured? |  |  |
| Describe the legal basis for processing under GDPR Article 6, and if applicable, the exemption under Article 9(2)(j) for scientific research or use of explicit consent Confirm the expected roles of data controller(s) and processor(s), including institutional and external parties State whether a Data Protection Impact Assessment (DPIA) is likely to be required, and when it will be conducted Describe initial plans for pseudonymization and/or anonymization Summarize the intended technical safeguards (e.g., secure storage, encryption, access control; see Section 3c) Reference relevant institutional support (e.g., contact with Data Protection Officer, planned ethics submission) Mention any planned data processing, use, or transfer agreements (e.g., DPA, DUA, SCCs) | Confirm that the legal basis for processing has been formally documented and, if applicable, that informed consent has been obtained or justified Confirm completion of a DPIA, if required Describe the technical and organizational measures actually implemented Identify the data storage locations and systems in use, confirming institutional approval and security standards (see Section 3c) List any signed agreements (e.g., DPA with core facilities, DUAs with collaborators, SCCs for international transfers) Describe how ethics approval, risk assessments, or external audits are being managed Update any changes to the scope of personal data processing or data sharing | Confirm that all data protection measures were implemented and monitored throughout the project lifecycle Describe how personal data have been securely deleted, anonymized, or retained in line with the approved retention schedule State whether any personal data will be reused and on what legal basis Confirm that any published outputs or deposited datasets have been anonymized or shared under appropriate controlled access conditions Describe how documentation, such as the DPIA, consent forms, and agreements, will be archived or made available for audit |
| 4b How will other legal issues, such as intellectual property rights and ownership, be managed? What legislation is applicable? |  |  |
| Identify elements that may involve legal issues or state that there are no legal restrictions Identify the anticipated data outputs (e.g., datasets, software, protocols) that may be subject to IPR Describe who will own the data and results Mention whether collaboration agreements, joint IP agreements, or employment contracts define ownership and access rights Summarize relevant legal frameworks (e.g., national copyright law, institutional IP policies, Nagoya Protocol) State whether IP and authorship will be handled by a technology transfer office (TTO) or legal department | Confirm that any relevant IP or data ownership clauses or other legal issues have been formalized in signed agreements (e.g., collaboration agreements, DUAs) Describe how access rights to data are managed internally and with partners Confirm who holds the right to publish results and reuse data after project completion Identify who is responsible for maintaining records of IP decisions or agreements | Confirm the final ownership status and other legal issues of data and outputs Describe how access and reuse of data will be managed post-project (e.g., open access, embargoed, restricted) Note that any restrictions to data sharing should be explained in Section 5a List any registered IP (e.g., software licenses, patents) resulting from the project Summarize how legal or contractual obligations were fulfilled regarding ownership and reuse Confirm that collaborators and institutions are aware of their ongoing rights or restrictions related to the data |
| Check if ethical issues are a major concern in the project (e.g., health or animal research) Describe the need for ethics approval from an ethical committee, and whether it has been applied for or is planned List the responsible person and check procedures if needed List the existing relevant codes of conduct or standards if needed | Confirm that ethics approval has been obtained, and list the approving body Summarize any changes to ethical considerations (e.g., new data sources, new populations) | Confirm ethical issues have been fulfilled and documented Confirm that any published data or materials comply with ethics approvals and participant agreements |
| 5   DATA SHARING AND LONG-TERM PRESERVATION 5a How and when will data be shared? Are there possible restrictions to data sharing or embargo reasons? |  |  |
| Provide a clear plan for data sharing, describing what data will be shared, where, and when (e.g., via a trusted repository upon publication or after a defined embargo) If data cannot be shared, provide a plan for sharing the metadata as an alternative If data cannot be shared, explain how the "good scientific practices" of storage for 10 years will be handled | Identify actual repositories used or selected Note any updates to sharing timelines or formats Ensure internal sharing of metadata and data regularly | Confirm datasets have been deposited with appropriate metadata, licenses, and PIDs; explain deviations Specify the license for reuse (e.g., CC BY, CC0) If data cannot be shared (e.g., for legal, ethical, or commercial reasons), explain valid restrictions and access conditions (see Section 4a) For sensitive data requiring restricted access, clearly state access conditions and procedures Specify the timing of data availability, including any embargo periods and justifications |
| 5b How will data for preservation be selected, and where will data be preserved long-term in a suitable archive? (for example a data repository or archive) |  |  |
| Describe the criteria for selecting which data and metadata will be preserved long-term (e.g., final validated data, high-impact datasets, representative samples) Name a specific repository or archive for long-term preservation If no repository is used, explain the storage plan in detail State the planned retention period (e.g., 10+ years), ensuring it aligns with funder or community expectations | Identify actual datasets being planned to be stored in the repositories; note any updates Report any updates on selection criteria or retention period | Confirm datasets have been deposited in the repository or archive with appropriate metadata, licenses, and PIDs; explain deviations |
| 5c What methods or software tools are needed to access and use data? |  |  |
| List the planned file formats used for preservation Prefer open, non-proprietary formats; if proprietary formats are used, acknowledge and justify this List the planned tools or software to access, visualize, or analyze the data | Update tools or software required to access, visualize, or analyze the data Prioritize open-source tools or recommend them where possible If proprietary tools are required, provide alternatives or rationale Update file formats used for preservation | Confirm file formats used for preservation are listed Prefer open, non-proprietary formats; if proprietary formats are used, acknowledge and justify this Recommendation: provide both proprietary and open format, if possible, for smaller datasets Confirm tools or software required to access, visualize, or analyze the data are named Describe access mechanisms clearly (e.g., via a GUI, API, or direct download) If applicable, include or link source code or workflows needed to interpret the data (e.g., analysis scripts, custom parsers, or notebooks), ideally under an open license |
| 5d How will the application of a unique and persistent identifier (such as a Digital Object Identifier (DOI)) to each data set be ensured? |  |  |
| Ensure the plan links persistent identifiers with metadata to support findability, citation, and reuse If no persistent identifiers are planned, provide justification (e.g., internal use, access limitations, early-stage data) |  | Provide a list of published datasets and their PIDs Ensure that identifiers are linked with metadata to support findability, citation, and reuse If no persistent identifiers are assigned, provide justification (e.g., internal use, access limitations, early-stage data) Note the automation of the identifiers |
| 6   DATA MANAGEMENT RESPONSIBILITIES AND RESOURCES 6a Who (for example role, position, and institution) will be responsible for data management (i.e. the data steward)? |  |  |
| List the roles and responsibilities of all relevant project stakeholders in relation to data management (e.g., principal investigator, researchers, data steward, IT staff) Assign responsibilities for key data-related tasks: data acquisition and storage, metadata creation and documentation, backup and recovery procedures, repository submission and long-term preservation, compliance with institutional/funder/legal data policies Address who will be responsible for maintaining access and managing data after the end of the project (e.g., institutional support, long-term repository) | Confirm actual roles and effort committed; monitor whether responsibilities are fulfilled in practice Identify any bottlenecks or overlaps in data-related tasks and adjust accordingly Ensure replacement or onboarding procedures for team members Update RDM documentation to reflect current team structure | Finalize and document the complete list of contributors and institutional support Reflect on the adequacy of assigned resources and responsibilities |
| 6b What resources (for example financial and time) will be dedicated to data management and ensuring that data will be FAIR (Findable, Accessible, Interoperable, Re-usable)? |  |  |
| Provide preliminary estimates of human, financial, and technical resources for data management (e.g., time commitments, infrastructure needs, repository fees, staff time in person-days or FTE) Identify planned external or institutional support (e.g., consortia services, RDM training) Mention plans for external support (e.g., from national infrastructures, consortia, or service providers), if applicable Acknowledge support from institutional infrastructure or services (e.g., data storage, backup systems, RDM training) | Review actual resource allocation and expenditures Monitor workload against estimates and adjust where necessary Assess sufficiency of storage, software, and training provisions Document any additional support acquired | Summarize total resources spent and their impact on data management effectiveness Evaluate cost-efficiency and adequacy of FAIR implementation Provide a final report on institutional and external support contributions |

# 4\. Discipline-Specific Guidance

The general criteria above are complemented by discipline-specific best practices and examples:

Materials Science: FAIRmat  
Plant Science: DataPLANT-DMP-Examples  
Additional disciplines: Community contributions welcome

# 5\. How to Use This Document

## For Data Stewards

Select evaluation criteria appropriate to the project phase  
Use as a checklist during DMP consultations  
Adapt to specific funder or institutional requirements

## For Researchers

Use as a guide when preparing DMPs  
Understand expectations at each project stage  
Plan resource allocation across the project lifecycle

## For Reviewers

Apply structured assessment framework  
Focus on phase-appropriate completeness  
Provide consistent, transparent evaluation

## For Funders

Adapt criteria to specific program requirements  
Ensure alignment with institutional policies  
Support standardized evaluation processes

## For AI Assisted Evaluation

# 6\. Contributing and Feedback

This is a living, community-driven resource. We welcome:

Discipline-specific guidance contributions  
Adaptations for specific funder requirements  
Feedback on criteria applicability and clarity

## Acknowledgments

Developed by the infra-dmp DMP Evaluation Criteria Working Group, building on the Science Europe DMP evaluation rubric.  
